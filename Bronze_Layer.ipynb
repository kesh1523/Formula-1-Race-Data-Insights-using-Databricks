{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99279b30-57e6-4104-ba08-e6311fa284c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "circuits, races, constructors, drivers -> data from all races ->Autoloader + merge into\n",
    "\n",
    "results, pitstops, laptimes, qualifying -> data from single/that race -> use autoloader \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a1c8c45-152c-4d44-9926-8d947436660a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "batch_files = ['circuits', 'races', 'constructors', 'drivers']\n",
    "for file in batch_files:\n",
    "    spark.sql(f'CREATE TABLE IF NOT EXISTS keshcatalog.bronze.{file}')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c59e7b78-d0cf-41aa-8c32-e1efd525489f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "incremental_files=['results', 'qualifying', 'pit_stops', 'lap_times']\n",
    "for file in incremental_files:\n",
    "    spark.sql(f'CREATE TABLE IF NOT EXISTS keshcatalog.bronze.{file}')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cee4e78-ded3-4919-b078-2334ab5af33f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_dir = 'abfss://raw@keshstorage09.dfs.core.windows.net/incremental'    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93a7cb6d-c8b7-4bda-aa71-a5851f501f2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "CIRCUITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5862e448-7e91-4b20-8ca3-270cfcdc4e87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType\n",
    "from pyspark.sql.functions import *\n",
    "circuits_schema = StructType([\n",
    "    StructField('circuitId', IntegerType(), True),\n",
    "    StructField('circuitRef', StringType(), True),\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('location', StringType(), True),\n",
    "    StructField('country', StringType(), True),\n",
    "    StructField('lat', FloatType(), True),\n",
    "    StructField('lng', FloatType(), True),\n",
    "    StructField('alt', IntegerType(), True),\n",
    "    StructField('url', StringType(), True)\n",
    "])\n",
    "\n",
    "circuits_df = (\n",
    "    spark.readStream\n",
    "    .format('cloudFiles')\n",
    "    .option('cloudFiles.format', 'csv')\n",
    "    .option('header', 'true')\n",
    "    .schema(circuits_schema)\n",
    "    .load(f'{base_dir}/*/circuits/')\n",
    ")\n",
    "circuits_df = circuits_df.withColumn('ingestion_timestamp',current_timestamp() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42aacf7f-b7e5-417e-8db7-0577ec213060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.streaming.query.StreamingQuery at 0x7fed8d990390>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "def mergeinto(microBatchDF, batchId):\n",
    "    microBatchDF = microBatchDF.dropDuplicates([\"circuitId\"])\n",
    "    target = DeltaTable.forName(spark, \"keshcatalog.bronze.circuits\")\n",
    "    (target.alias(\"t\")\n",
    "        .merge(\n",
    "            microBatchDF.alias(\"s\"),\n",
    "            \"t.circuitId = s.circuitId\"\n",
    "        )\n",
    "        .whenMatchedUpdateAll()\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "query=(circuits_df.writeStream\n",
    "   .foreachBatch(mergeinto)\n",
    "   .option(\"checkpointLocation\", \"abfss://raw@keshstorage09.dfs.core.windows.net/checkpoints/circuits\")\n",
    "   .option(\"mergeSchema\", \"true\")\n",
    "   .trigger(once=True)\n",
    "   .start()\n",
    ")   \n",
    "query.awaitTermination()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbbba5a9-2131-4c24-b180-2327b51e21d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "RACES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ac1760e-d0ec-424d-afef-e4c8dfaaf3f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS keshcatalog.bronze.races_ (\n",
    "  raceId INT,\n",
    "  year INT,\n",
    "  round INT,\n",
    "  circuitId INT,\n",
    "  name STRING,\n",
    "  date DATE,\n",
    "  time STRING,\n",
    "  url STRING\n",
    ") USING delta;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e75e57b0-c019-4ec8-8d52-dfc86c299998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType\n",
    "races_schema = StructType([\n",
    "    StructField('raceId', IntegerType(), True),\n",
    "    StructField('year', IntegerType(), True),\n",
    "    StructField('round', IntegerType(), True),\n",
    "    StructField('circuitId', IntegerType(), True),\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('date', DateType(), True),\n",
    "    StructField('time', StringType(), True),\n",
    "    StructField('url', StringType(), True)\n",
    "])\n",
    "\n",
    "races_df = (\n",
    "    spark.readStream\n",
    "    .format('cloudFiles')\n",
    "    .option('cloudFiles.format', 'csv')\n",
    "    .option('header', 'true')\n",
    "    .schema(races_schema)\n",
    "    .load(f'{base_dir}/*/races/')\n",
    ")\n",
    "races_df = races_df.withColumn('ingestion_timestamp',current_timestamp() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cc2b71a-eb1a-49e9-93ce-7168aabe38c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.streaming.query.StreamingQuery at 0x7fedb53b22d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "def mergeinto(microBatchDF, batchId):\n",
    "    microBatchDF = microBatchDF.dropDuplicates([\"raceId\"])\n",
    "    target = DeltaTable.forName(spark, \"keshcatalog.bronze.races_\")\n",
    "\n",
    "    for col in microBatchDF.columns:\n",
    "        if col.lower() in [c.lower() for c in target.toDF().columns]:\n",
    "            microBatchDF = microBatchDF.withColumnRenamed(col, col.lower())\n",
    "    \n",
    "    (target.alias(\"t\")\n",
    "        .merge(\n",
    "            microBatchDF.alias(\"s\"),\n",
    "            \"t.raceid = s.raceid\"   \n",
    "        )\n",
    "        .whenMatchedUpdateAll()\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "\n",
    "query=(\n",
    "    races_df.writeStream\n",
    "        .foreachBatch(mergeinto)\n",
    "        .option(\"checkpointLocation\", \"abfss://raw@keshstorage09.dfs.core.windows.net/checkpoints/races\")\n",
    "        .trigger(once=True)\n",
    "        .start()\n",
    ")\n",
    "query.awaitTermination()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45f9044a-52e8-46d4-ac3f-161ce418e7f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "CONSTRUCTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26e728e4-5093-4beb-9bdb-4d230e316a8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "constructors_schema = StructType([\n",
    "    StructField('constructorId', IntegerType(), True),\n",
    "    StructField('constructorRef', StringType(), True),\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('nationality', StringType(), True),\n",
    "    StructField('url', StringType(), True)\n",
    "])\n",
    "\n",
    "constructors_df = (\n",
    "    spark.readStream\n",
    "    .format('cloudFiles')\n",
    "    .option('cloudFiles.format','json')\n",
    "    .option('header', 'true')\n",
    "    .schema(constructors_schema)\n",
    "    .load(f'{base_dir}/*/constructors')\n",
    ")\n",
    "constructors_df = constructors_df.withColumn('ingestion_timestamp',current_timestamp() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0793e1f5-b1ac-4dae-868b-100996ffb373",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.streaming.query.StreamingQuery at 0x7f6687cd7a90>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mergeinto(microBatchDF, batchId):\n",
    "    microBatchDF=microBatchDF.dropDuplicates([\"constructorId\"])\n",
    "    target = DeltaTable.forName(spark, \"keshcatalog.bronze.constructors\")\n",
    "    (target.alias(\"t\")\n",
    "        .merge(\n",
    "            microBatchDF.alias(\"s\"),\n",
    "            \"t.constructorId = s.constructorId\"\n",
    "        )\n",
    "        .whenMatchedUpdateAll()\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "\n",
    "query=(constructors_df.writeStream\n",
    ".foreachBatch(mergeinto)\n",
    ".option('checkpointLocation','abfss://raw@keshstorage09.dfs.core.windows.net/checkpoints/constructors')\n",
    ".option(\"mergeSchema\", \"true\")\n",
    ".trigger(once=True)\n",
    ".start()\n",
    ")\n",
    "query.awaitTermination()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0230d0ee-e4f6-4730-a637-34726078dc97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DRIVERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7db97a1e-9f4b-4414-8f64-99222ff6fabe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "\n",
    "\n",
    "name_schema = StructType([\n",
    "    StructField(\"forename\", StringType(), True),\n",
    "    StructField(\"surname\", StringType(), True)\n",
    "])\n",
    "\n",
    "driver_schema = StructType([\n",
    "    StructField(\"driverId\", IntegerType(), True),\n",
    "    StructField(\"driverRef\", StringType(), True),\n",
    "    StructField(\"number\", StringType(), True),\n",
    "    StructField(\"code\", StringType(), True),\n",
    "    StructField(\"name\", name_schema, True),   \n",
    "    StructField(\"dob\", DateType(), True),\n",
    "    StructField(\"nationality\", StringType(), True),\n",
    "    StructField(\"url\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "drivers_df_nested = (spark.readStream\n",
    "    .format(\"cloudFiles\") \n",
    "    .option('cloudFiles.format','json')\n",
    "    .option('header','true')\n",
    "    .schema(driver_schema) \n",
    "    .load(f\"{base_dir}/*/drivers\"))\n",
    "\n",
    "\n",
    "drivers_df = drivers_df_nested.select(\n",
    "    \"driverId\",\n",
    "    \"driverRef\",\n",
    "    \"number\",\n",
    "    \"code\",\n",
    "    \"name.forename\",\n",
    "    \"name.surname\",\n",
    "    \"dob\",\n",
    "    \"nationality\",\n",
    "    \"url\"\n",
    ")\n",
    "drivers_df = drivers_df.withColumn('ingestion_timestamp',current_timestamp() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b5b1196-fe5a-4d48-819e-ba2491814458",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.streaming.query.StreamingQuery at 0x7f66b4fe9310>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mergeinto(microBatchDF, batchId):\n",
    "    microBatchDF=microBatchDF.dropDuplicates([\"driverId\"])\n",
    "    target = DeltaTable.forName(spark, \"keshcatalog.bronze.drivers\")  \n",
    "    (target.alias(\"t\")\n",
    "        .merge(\n",
    "            microBatchDF.alias(\"s\") ,\n",
    "            \"t.driverId = s.driverId\"\n",
    "        )        \n",
    "        .whenMatchedUpdateAll()\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "\n",
    "query=(drivers_df.writeStream\n",
    ".foreachBatch(mergeinto)\n",
    ".option('checkpointLocation','abfss://raw@keshstorage09.dfs.core.windows.net/checkpoints/drivers')\n",
    ".option(\"mergeSchema\", \"true\")\n",
    ".trigger(once=True)\n",
    ".start()\n",
    ")\n",
    "query.awaitTermination()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa77e4fd-14eb-47f3-a15e-5684a5569c29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "RESULTS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de6ee05b-262a-4583-9a5c-1fcb6ad04f3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "\n",
    "\n",
    "date = '2021-03-21'\n",
    "base_dir = 'abfss://raw@keshstorage09.dfs.core.windows.net/incremental'\n",
    "\n",
    "\n",
    "results_schema = StructType([\n",
    "    StructField(\"resultId\", IntegerType(), True),\n",
    "    StructField(\"raceId\", IntegerType(), True),\n",
    "    StructField(\"driverId\", IntegerType(), True),\n",
    "    StructField(\"constructorId\", IntegerType(), True),\n",
    "    StructField(\"number\", IntegerType(), True),\n",
    "    StructField(\"grid\", IntegerType(), True),\n",
    "    StructField(\"position\", IntegerType(), True),\n",
    "    StructField(\"positionText\", StringType(), True),\n",
    "    StructField(\"positionOrder\", IntegerType(), True),\n",
    "    StructField(\"points\", FloatType(), True),\n",
    "    StructField(\"laps\", IntegerType(), True),\n",
    "    StructField(\"time\", StringType(), True),\n",
    "    StructField(\"milliseconds\", IntegerType(), True),\n",
    "    StructField(\"fastestLap\", IntegerType(), True),\n",
    "    StructField(\"rank\", IntegerType(), True),\n",
    "    StructField(\"fastestLapTime\", StringType(), True),\n",
    "    StructField(\"fastestLapSpeed\", FloatType(), True),\n",
    "    StructField(\"statusId\", IntegerType(), True)\n",
    "])\n",
    "results_df = spark.readStream.format('cloudFiles').option('cloudFiles.format', 'json').option('header','true').schema(results_schema).load(f'{base_dir}/*/results/')\n",
    "results_df = results_df.withColumn('ingestion_timestamp',current_timestamp() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71e695c1-67db-47f8-a076-60d3ea4857b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.streaming.query.StreamingQuery at 0x7f66b3a37c10>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=(results_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\n",
    "        \"checkpointLocation\",\n",
    "        \"abfss://raw@keshstorage09.dfs.core.windows.net/checkpoints/results/\"\n",
    "    ) \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(once=True) \\\n",
    "    .table(\"keshcatalog.bronze.results\") )\n",
    "query.awaitTermination()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33cac610-dbe8-4ef1-a60f-bbf96b8706be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "LAP TIMES DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aea4ea3-68ef-4c6a-85c9-5468da13b723",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "laps_times_schema = StructType([\n",
    "    StructField(\"raceId\", IntegerType(), True),\n",
    "    StructField(\"driverId\", IntegerType(), True),\n",
    "    StructField(\"lap\", IntegerType(), True),\n",
    "    StructField(\"position\", IntegerType(), True),\n",
    "    StructField(\"time\", StringType(), True),\n",
    "    StructField(\"milliseconds\", IntegerType(), True)\n",
    "]) \n",
    "laps_times_df = spark.readStream.format('cloudFiles').option('cloudFiles.format','csv').schema(laps_times_schema).load(f'{base_dir}/*/lap_times/')\n",
    "laps_times_df = laps_times_df.withColumn('ingestion_timestamp',current_timestamp() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d2ad10e-eab8-4f5b-a5d1-50bc3dc22252",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.streaming.query.StreamingQuery at 0x7f66865eac90>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=laps_times_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\n",
    "        \"checkpointLocation\",\n",
    "        \"abfss://raw@keshstorage09.dfs.core.windows.net/checkpoints/lap_times/\"\n",
    "    ) \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(once=True) \\\n",
    "    .table(\"keshcatalog.bronze.lap_times\")\n",
    "query.awaitTermination() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "896f9445-f7e8-44df-a6b3-0f8b5ea92ded",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "PIT STOPS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6467a36d-227b-4344-b79d-232c8d3d4a33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "\n",
    "pit_stops_schema = StructType([\n",
    "    StructField(\"raceId\", IntegerType(), True),\n",
    "    StructField(\"driverId\", IntegerType(), True),\n",
    "    StructField(\"stop\", IntegerType(), True),\n",
    "    StructField(\"lap\", IntegerType(), True),\n",
    "    StructField(\"time\", StringType(), True),\n",
    "    StructField(\"duration\", FloatType(), True),\n",
    "    StructField(\"milliseconds\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "pit_stops_df = spark.readStream.format('cloudFiles').option('cloudFiles.format', 'json').option('header','true').option('multiline','true').schema(pit_stops_schema).load(f'{base_dir}/*/pit_stops/')\n",
    "pit_stops_df = pit_stops_df.withColumn('ingestion_timestamp',current_timestamp() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c8dcdd6-0352-4300-8920-eb9b90561d77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.streaming.query.StreamingQuery at 0x7f6686403ad0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=pit_stops_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\n",
    "        \"checkpointLocation\",\n",
    "        \"abfss://raw@keshstorage09.dfs.core.windows.net/checkpoints/pit_stops/\"\n",
    "    ) \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(once=True) \\\n",
    "    .table(\"keshcatalog.bronze.pit_stops\")\n",
    "query.awaitTermination() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "338b480c-6012-4ea4-8c72-4a45a583e280",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "QUALIFYING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8c0fdb5-88b2-4d31-958f-c635773911a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-5341728883069491>, line 1\u001b[0m\n",
       "\u001b[0;32m----> 1\u001b[0m qualifying_schema \u001b[38;5;241m=\u001b[39m StructType([\n",
       "\u001b[1;32m      2\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqualifyId\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n",
       "\u001b[1;32m      3\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraceId\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n",
       "\u001b[1;32m      4\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriverId\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n",
       "\u001b[1;32m      5\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstructorId\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n",
       "\u001b[1;32m      6\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n",
       "\u001b[1;32m      7\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposition\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n",
       "\u001b[1;32m      8\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq1\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n",
       "\u001b[1;32m      9\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq2\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n",
       "\u001b[1;32m     10\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq3\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
       "\u001b[1;32m     11\u001b[0m ])\n",
       "\u001b[1;32m     12\u001b[0m qualifying_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mreadStream\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcloudFiles\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcloudFiles.format\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultiline\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mschema(qualifying_schema)\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/*/qualifying\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
       "\u001b[1;32m     13\u001b[0m qualifying_df\u001b[38;5;241m=\u001b[39mqualifying_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mingestion_timestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, current_timestamp())\n",
       "\n",
       "\u001b[0;31mNameError\u001b[0m: name 'StructType' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "NameError",
        "evalue": "name 'StructType' is not defined"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'StructType' is not defined"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "File \u001b[0;32m<command-5341728883069491>, line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m qualifying_schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[1;32m      2\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqualifyId\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      3\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraceId\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      4\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriverId\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      5\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstructorId\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      6\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      7\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposition\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      8\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq1\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      9\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq2\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     10\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq3\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m ])\n\u001b[1;32m     12\u001b[0m qualifying_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mreadStream\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcloudFiles\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcloudFiles.format\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultiline\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mschema(qualifying_schema)\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/*/qualifying\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m qualifying_df\u001b[38;5;241m=\u001b[39mqualifying_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mingestion_timestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, current_timestamp())\n",
        "\u001b[0;31mNameError\u001b[0m: name 'StructType' is not defined"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "qualifying_schema = StructType([\n",
    "    StructField(\"qualifyId\", IntegerType(), True),\n",
    "    StructField(\"raceId\", IntegerType(), True),\n",
    "    StructField(\"driverId\", IntegerType(), True),\n",
    "    StructField(\"constructorId\", IntegerType(), True),\n",
    "    StructField(\"number\", IntegerType(), True),\n",
    "    StructField(\"position\", IntegerType(), True),\n",
    "    StructField(\"q1\", StringType(), True),\n",
    "    StructField(\"q2\", StringType(), True),\n",
    "    StructField(\"q3\", StringType(), True)\n",
    "])\n",
    "qualifying_df = spark.readStream.format('cloudFiles').option('cloudFiles.format', 'json').option('header','true').option('multiline','true').schema(qualifying_schema).load(f'{base_dir}/*/qualifying')\n",
    "qualifying_df = qualifying_df.withColumn('ingestion_timestamp',current_timestamp() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da6fe667-1575-4f90-98b7-526e18b10333",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.streaming.query.StreamingQuery at 0x7f6687cba3d0>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=qualifying_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\n",
    "        \"checkpointLocation\",\n",
    "        \"abfss://raw@keshstorage09.dfs.core.windows.net/checkpoints/qualifying/\"\n",
    "    ) \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(once=True) \\\n",
    "    .table(\"keshcatalog.bronze.qualifying\")\n",
    "query.awaitTermination() "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5096030356534936,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
